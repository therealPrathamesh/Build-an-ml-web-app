{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "engaged-extreme",
   "metadata": {},
   "source": [
    "# Path 3 - Building and Testing A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-collection",
   "metadata": {},
   "source": [
    "What is machine learning?\n",
    "1. Supervised Learning\n",
    "    - We give out model questions and answers\n",
    "    - e.g. is this a picture of a cat or dog?\n",
    "2. Unsupervised Learning\n",
    "    - We give out model unlabeled data, and it figures out something about it\n",
    "    - e.g. what are the most common types of customers I have?\n",
    "3. Reinforcement Learning\n",
    "    - We give our model an environment to play in, and a notion of when it wins or looses\n",
    "    - e.g. playing chess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-controversy",
   "metadata": {},
   "source": [
    "Our case is definitely supervised learning--we have questions and answers (question: how much should a house cost that has these features? answer: the price!). So a model is anything that takes those details, and infers the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('https://raw.githubusercontent.com/wlifferth/build-an-ml-web-app/main/cleaned_data.csv', index_col='id')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_price = train_df['price'].mean()\n",
    "\n",
    "average_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model_df = train_df.copy()\n",
    "\n",
    "mean_model_df['predicted'] = 335720\n",
    "\n",
    "mean_model_df['absolute_error'] = np.abs(mean_model_df['price'] - mean_model_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_model_df['absolute_error'])\n",
    "\n",
    "mean_model_df['absolute_error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is literally the simplest model we could build--and this counts as a model! Just a really dumb one : )\n",
    "\n",
    "# 4. How do we make a model more powerful?\n",
    "#     a. More data\n",
    "#         i. Take into account more variables (i.e. what's the average price of a square foot)\n",
    "#         ii. Add more rows to our dataset\n",
    "#     b. More \"capacity\"\n",
    "#         i. Give our model a bigger brain (we'll look at this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we incorporated just the livingArea value?\n",
    "# What is the average price per square foot?\n",
    "\n",
    "square_footage_model_df = train_df.copy()\n",
    "square_footage_model_df['price_per_sqft'] = square_footage_model_df['price'] / square_footage_model_df['livingArea']\n",
    "\n",
    "square_footage_model_df['price_per_sqft'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_footage_model_df['predicted'] = square_footage_model_df['livingArea'] * 195.35527446966\n",
    "\n",
    "square_footage_model_df['absolute_error'] = np.abs(square_footage_model_df['price'] - square_footage_model_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(square_footage_model_df['absolute_error'])\n",
    "\n",
    "square_footage_model_df['absolute_error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow! We just made our model a lot more accurate! On average, we're 20k closer to the correct price!\n",
    "# 5. This is our first model--it's called linear regression\n",
    "#     a. sci-kit learn lets us build this kind of model quickly!\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg_df = train_df.copy()\n",
    "\n",
    "input_data = lin_reg_df[['livingArea']] # This is 2d\n",
    "output_data = lin_reg_df['price'] # This is 1d\n",
    "\n",
    "linear_regression_on_living_area_model = LinearRegression()\n",
    "\n",
    "linear_regression_on_living_area_model.fit(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_df['predicted'] = linear_regression_on_living_area_model.predict(input_data)\n",
    "\n",
    "lin_reg_df['absolute_error'] = np.abs(lin_reg_df['price'] - lin_reg_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lin_reg_df['absolute_error'])\n",
    "\n",
    "# Woah--it did a little bit better than us--what's going on? (Adding a bias)\n",
    "lin_reg_df['absolute_error'].mean(skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now would also be a good time to introduce another helpful utility from scikit learn--calculating our error for us:\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = linear_regression_on_living_area_model.predict(input_data)\n",
    "\n",
    "mean_absolute_error(lin_reg_df['price'], predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also give our model more capacity\n",
    "lin_reg_df = train_df.copy() # Overwriting lin_reg_df\n",
    "\n",
    "lin_reg_df['livingAreaSquared'] = lin_reg_df['livingArea'] ** 2\n",
    "lin_reg_df['livingAreaRooted'] = lin_reg_df['livingArea'] ** 0.5\n",
    "\n",
    "input_data = lin_reg_df[['livingArea', 'livingAreaSquared', 'livingAreaRooted']]\n",
    "output_data = lin_reg_df['price']\n",
    "lr_on_living_area_nonlinear_model = LinearRegression()\n",
    "lr_on_living_area_nonlinear_model.fit(input_data, output_data)\n",
    "predictions = lr_on_living_area_nonlinear_model.predict(input_data)\n",
    "mean_absolute_error(lin_reg_df['price'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay--we got a little bit better! Could we just keep adding additional terms?\n",
    "columns = ['livingArea', 'livingAreaSquared', 'livingAreaRooted']\n",
    "for i in range(2,5):\n",
    "    column = f'livingAreaToThePowerOf{i}'\n",
    "    columns.append(column)\n",
    "    lin_reg_df[column] = lin_reg_df['livingArea'] ** i\n",
    "\n",
    "input_data = lin_reg_df[columns]\n",
    "output_data = lin_reg_df['price']\n",
    "lr_on_living_area_nonlinear_model = LinearRegression()\n",
    "lr_on_living_area_nonlinear_model.fit(input_data, output_data)\n",
    "predictions = lr_on_living_area_nonlinear_model.predict(input_data)\n",
    "mean_absolute_error(lin_reg_df['price'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technically it's vbetter, but not by much\n",
    "# 8. Now it's time for the big butt in machine learning, and it's called over fitting. I've spent a lot of time thnking about what the best way to explain over fitting is, and I think a really good analogy is with study guides.\n",
    "#     a. So I want us to all pretend that I'm a biology teacher, and you all are my students.\n",
    "#     b. I have 100 questions I've come up with that cover our material, and I need to make a test, and give y'all a study guide.\n",
    "#     c. So lets say I give you all 100 questions, with answers, as the study guide. Then I randomly pick 10 of them to be the test.\n",
    "#     d. This might be a fine way of doing things. But, what if some of my students have a photographic memory? This is when someone can look at something, and basically without thinking, recall every specifc detail of what they saw. This is kind of the equivalent of a high-capacity model.\n",
    "#     e. Well this would be bad, because the students wouldn't have to learn anything, they could just memorize the specific questions and regugitate them.\n",
    "#     f. This is one of the biggest problems we face in machine learning--it's called over fitting. And the easiest way to think about it, is when your model just memorizes the training data.\n",
    "#     g. Why is this a problem? Because it doesn't generalize--you can only perform well on data you have already seen. So you can't actually make good predictions.\n",
    "#     h. So what would we do in the study guide example?\n",
    "#     i. I could take my 100 questions, give 90 of them to you as a study guide, and keep the remaining 10 a secret for the test. That way you can't get a high grade just by memorizing, you actually have to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.DataFrame({\n",
    "    'x': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "    # 'y': [-3, 14, 16, 9, 12, 14, 39, 63]\n",
    "    'y': [0.0, 1.0, 1.4142135623730951, 2.6, 2.0, 2.23606797749979, 2.449489742783178, 2.6457513110645907, 2.8284271247461903, 2.5, 3.1622776601683795, 3.3166247903554]\n",
    "})\n",
    "\n",
    "plt.scatter(fake_data['x'], fake_data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['x']\n",
    "predicted_columns = []\n",
    "for i in range(1,10):\n",
    "    column = f'xToThePowerOf{i}'\n",
    "    columns.append(column)\n",
    "    fake_data[column] = fake_data['x'] ** i\n",
    "    model = LinearRegression()\n",
    "    model.fit(fake_data[columns], fake_data['y'])\n",
    "    predicted_column = f'predictedFrom{i}'\n",
    "    predicted_columns.append(predicted_column)\n",
    "    fake_data[predicted_column] = model.predict(fake_data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "for predicted_column in ['predictedFrom1', 'predictedFrom2', 'predictedFrom5', 'predictedFrom9']:\n",
    "    plt.title(predicted_column)\n",
    "    plt.scatter(fake_data['x'], fake_data['y'])\n",
    "    plt.plot(fake_data[predicted_column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So how do we make sure we're not making that last model?\n",
    "# Thinking back to our story about tests and study guides, we can do the same thing.\n",
    "# 9. When we do this in machine learning, it's called cross validation.\n",
    "#     a. Basically we split our training data up into a smaller training set the model gets to see, then we test it on the rest of the data it hasn't seen yet.\n",
    "# 10. So now we see that adding capacity helps, up to a point. If we add too much capacity, our model just starts memorizings things, and does't perform as well on the data.from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df[['livingArea']]\n",
    "y = train_df['price']\n",
    "\n",
    "errors = []\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=i)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    error = mean_absolute_error(predictions, y_test)\n",
    "    print(error)\n",
    "    errors.append(error)\n",
    "print(f'Mean Error {np.mean(errors)}')\n",
    "\n",
    "# Already we see that our error is worse when our model is being tested on data it hasn't seen yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we've eaten our vegetables, now we get to go nuts--lets throw in all the data we cleaned last time!\n",
    "\n",
    "X = train_df.drop(['city', 'state', 'lotUnit', 'price'], axis=1)\n",
    "y = train_df['price']\n",
    "\n",
    "errors = []\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=i)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    error = mean_absolute_error(predictions, y_test)\n",
    "    print(error)\n",
    "    errors.append(error)\n",
    "print(f'Mean Error {np.mean(errors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we one-hot encoded state?\n",
    "X = pd.get_dummies(train_df.drop(['city', 'lotUnit', 'price'], axis=1), columns=['state'])\n",
    "y = train_df['price']\n",
    "\n",
    "errors = []\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=i)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    error = mean_absolute_error(predictions, y_test)\n",
    "    print(error)\n",
    "    errors.append(error)\n",
    "print(f'Mean Error {np.mean(errors)}')\n",
    "\n",
    "# Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this has been using our original model, LinearRegression, but there are a lot of hot sexy models out there\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "X = pd.get_dummies(train_df.drop(['city', 'lotUnit', 'price'], axis=1), columns=['state'])\n",
    "y = train_df['price']\n",
    "\n",
    "errors = []\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=i)\n",
    "    model = MLPRegressor(hidden_layer_sizes=(4,))\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    error = mean_absolute_error(predictions, y_test)\n",
    "    print(error)\n",
    "    errors.append(error)\n",
    "print(f'Mean Error {np.mean(errors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, lets cover how to submit to the kaggle competition\n",
    "\n",
    "final_model = LinearRegression()\n",
    "\n",
    "final_training_input = pd.get_dummies(train_df.drop(['city', 'lotUnit', 'price'], axis=1), columns=['state'])\n",
    "\n",
    "X = pd.get_dummies(final_training_input)\n",
    "y = train_df['price']\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to do all the preprocessing we did on our training dataset on our testing dataset:\n",
    "\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/wlifferth/build-an-ml-web-app/main/test.csv', index_col='id')\n",
    "\n",
    "test.drop(['homeStatus', 'dateSold', 'address'], axis=1, inplace=True)\n",
    "\n",
    "def convert_lot_area(row):\n",
    "    if row['lotUnit'] == 'acres':\n",
    "        return row['lotArea'] * 43560\n",
    "    else:\n",
    "        return row['lotArea']\n",
    "\n",
    "test['lotArea'] = test.apply(convert_lot_area, axis=1)\n",
    "\n",
    "test.drop(['lotUnit'], inplace=True, axis=1)\n",
    "\n",
    "test = pd.get_dummies(test, columns=['homeType'])\n",
    "\n",
    "print(test.head())\n",
    "\n",
    "zip_code_df = pd.read_csv('median_income_by_zip_code.csv')\n",
    "\n",
    "zip_code_df['median_income']\n",
    "\n",
    "test = pd.merge(test, zip_code_df, how='left', left_on='zipcode', right_on='zip_code').set_index(test.index)\n",
    "\n",
    "test['median_income'].fillna(test['median_income'].mean(), inplace=True)\n",
    "\n",
    "test.drop(['zipcode', 'zip_code'], axis=1, inplace=True)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input = pd.get_dummies(test.drop(['city'], axis=1), columns=['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test['price'] = final_model.predict(final_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['price'].to_csv('2021-01-13-submission.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-insulin",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. One of the reasons our neural network didn't perform very well is because we didn't _normalize_ our data\n",
    "    - Basically neural network work best when all their inputs are of a similar magnitude, so we scale all our numbers down to be between -1 and 1\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "2. There are other cool ways of encoding our categorical data\n",
    "    - You could replace each city with the average house price of that city\n",
    "3. There are a ton of other cool models out there \n",
    "    - Search for regression on https://scikit-learn.org/stable/supervised_learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
